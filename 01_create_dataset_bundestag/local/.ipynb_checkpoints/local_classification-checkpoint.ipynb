{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb364636-95c6-49d8-9ff3-6ec22f8cc037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, AwqConfig\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09ca8ab-0c06-4eb1-b377-20e74ca983e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_total = 4\n",
    "i_index = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff1864a-b446-4ffd-adea-8dfc243a71d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_gpu = int(i_index%4)\n",
    "max_memory={0: \"48GB\", 1: \"0GB\", 2: \"0GB\", 3: \"0GB\"}\n",
    "if i_gpu == 0:\n",
    "    max_memory={0: \"48GB\", 1: \"0GB\", 2: \"0GB\", 3: \"0GB\"}\n",
    "elif i_gpu == 1:\n",
    "    max_memory={0: \"0GB\", 1: \"48GB\", 2: \"0GB\", 3: \"0GB\"}\n",
    "elif i_gpu == 2:\n",
    "    max_memory={0: \"0GB\", 1: \"0GB\", 2: \"48GB\", 3: \"0GB\"}\n",
    "elif i_gpu == 3:\n",
    "    max_memory={0: \"0GB\", 1: \"0GB\", 2: \"0GB\", 3: \"48GB\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e3db50-b2ea-45fa-844a-6fbdc80bfac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"hugging-quants/Meta-Llama-3.1-70B-Instruct-GPTQ-INT4\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_id,\n",
    "  torch_dtype=torch.float16,\n",
    "  low_cpu_mem_usage=True,\n",
    "  device_map=\"auto\",\n",
    "  max_memory=max_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb9ec61-9f85-4c65-8b8b-7c1203a8c9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_id = \"hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4\"\n",
    "#quantization_config = AwqConfig(\n",
    "#    bits=4,\n",
    "#    #fuse_max_seq_len=512, # Note: Update this as per your use-case\n",
    "#    fuse_max_seq_len=4096, # Note: Update this as per your use-case\n",
    "#    do_fuse=True,\n",
    "#)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "#model = AutoModelForCausalLM.from_pretrained(\n",
    "#  model_id,\n",
    "#  torch_dtype=torch.float16,\n",
    "#  low_cpu_mem_usage=True,\n",
    "#  device_map=\"auto\",\n",
    "#  quantization_config=quantization_config,\n",
    "#  max_memory=max_memory,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9399fa-3fc1-4225-bf22-4ac274f24c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype = torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf3e388-7d00-48a7-a52e-61dd0b0f2f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"SinclairSchneider/Bundestagsreden_senti_pos_neg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a144f2d6-5566-4c8c-ac4a-8406cabb7f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds['train'].to_pandas()\n",
    "df['index'] = list(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e456d26-af14-4497-97da-3048cf94c488",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_index_bool = [((x-i_index)%i_total)==0 for x in range(len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8ba97d-0e8b-4bc2-a882-19419b3b8734",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_part = df[l_index_bool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3449de60-d418-49ef-acc3-d61b16046206",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_Text = list(df_part['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e09d373-f161-4dfa-9c69-9892d295b2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = l_Text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc67412-23dc-41fe-98e3-061086048a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_kind = \"Fasse die folgende Rede in 1 bis 5 Sätzen so zusammen, dass ein Kind sie verstehen kann. Beginne nicht immer mit \\\"Es wird\\\" oder \\\"Der Redner\\\"!!! Schreibe die Sätze in der passiven Form. Erwähne nur den Inhalt, nicht wer was gesagt oder gefordert hat. Erwähne auch keine Parteien oder Personen. Konzentriere dich auf Forderungen, aber beginne nicht immer mit \\\"es wird gefordert\\\". Beginne die Zusammenfassung mit Zusammenfassung:\\n\"+text\n",
    "output_kind = llm_pipeline([{\"role\": \"user\", \"content\": prompt_kind}], max_new_tokens=512, temperature=1.0, )\n",
    "response_kind = output_kind[0]['generated_text'][1]['content']\n",
    "summary_kind = response_kind.replace(\"Zusammenfassung:\", \"\").strip()\n",
    "print(summary_kind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f425ac8-5180-4000-8e31-f72e7011c3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_jugendlicher = \"Fasse die folgende Rede in 1 bis 5 Sätzen so zusammen, dass ein Jugendlicher sie verstehen kann. Beginne nicht immer mit \\\"Es wird\\\" oder \\\"Der Redner\\\"!!! Schreibe die Sätze in der passiven Form. Erwähne nur den Inhalt, nicht wer was gesagt oder gefordert hat. Erwähne auch keine Parteien oder Personen. Konzentriere dich auf Forderungen, aber beginne nicht immer mit \\\"es wird gefordert\\\". Beginne die Zusammenfassung mit Zusammenfassung:\\n\"+text\n",
    "output_jugendlicher = llm_pipeline([{\"role\": \"user\", \"content\": prompt_jugendlicher}], max_new_tokens=512, temperature=1.0, )\n",
    "response_jugendlicher = output_jugendlicher[0]['generated_text'][1]['content']\n",
    "summary_jugendlicher = response_jugendlicher.replace(\"Zusammenfassung:\", \"\").strip()\n",
    "print(summary_jugendlicher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f36d954-2711-4d42-9a9f-751604d769eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_erwachsener = \"Fasse die folgende Rede in 1 bis 5 Sätzen so zusammen, dass ein Erwachsener sie verstehen kann. Beginne nicht immer mit \\\"Es wird\\\" oder \\\"Der Redner\\\"!!! Schreibe die Sätze in der passiven Form. Erwähne nur den Inhalt, nicht wer was gesagt oder gefordert hat. Erwähne auch keine Parteien oder Personen. Konzentriere dich auf Forderungen, aber beginne nicht immer mit \\\"es wird gefordert\\\". Beginne die Zusammenfassung mit Zusammenfassung:\\n\"+text\n",
    "output_erwachsener = llm_pipeline([{\"role\": \"user\", \"content\": prompt_erwachsener}], max_new_tokens=512, temperature=1.0, )\n",
    "response_erwachsener = output_erwachsener[0]['generated_text'][1]['content']\n",
    "summary_erwachsener = response_erwachsener.replace(\"Zusammenfassung:\", \"\").strip()\n",
    "print(summary_erwachsener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e869611a-d202-4c05-a1a4-c413a8d68afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_eloquenter_mensch = \"Fasse die folgende Rede in 1 bis 5 Sätzen so zusammen, dass ein sehr eloquenter Mensch, der sich durch eine gehobene Wortwahl abzugrenzen versucht, sie verstehen kann. Beginne nicht immer mit \\\"Es wird\\\" oder \\\"Der Redner\\\"!!! Schreibe die Sätze in der passiven Form. Erwähne nur den Inhalt, nicht wer was gesagt oder gefordert hat. Erwähne auch keine Parteien oder Personen. Konzentriere dich auf Forderungen, aber beginne nicht immer mit \\\"es wird gefordert\\\". Beginne die Zusammenfassung mit Zusammenfassung:\\n\"+text\n",
    "output_eloquenter_mensch = llm_pipeline([{\"role\": \"user\", \"content\": prompt_eloquenter_mensch}], max_new_tokens=512, temperature=1.0, )\n",
    "response_eloquenter_mensch = output_eloquenter_mensch[0]['generated_text'][1]['content']\n",
    "summary_eloquenter_mensch = response_eloquenter_mensch.replace(\"Zusammenfassung:\", \"\").strip()\n",
    "print(summary_eloquenter_mensch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b214a7d-675c-454a-8c07-c0bc5fcb8012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summaries(text):\n",
    "    prompt_kind = \"Fasse die folgende Rede in 1 bis 5 Sätzen so zusammen, dass ein Kind sie verstehen kann. Beginne nicht immer mit \\\"Es wird\\\" oder \\\"Der Redner\\\"!!! Schreibe die Sätze in der passiven Form. Erwähne nur den Inhalt, nicht wer was gesagt oder gefordert hat. Erwähne auch keine Parteien oder Personen. Konzentriere dich auf Forderungen, aber beginne nicht immer mit \\\"es wird gefordert\\\". Beginne die Zusammenfassung mit Zusammenfassung:\\n\"+text\n",
    "    output_kind = llm_pipeline([{\"role\": \"user\", \"content\": prompt_kind}], max_new_tokens=512, temperature=0.9, )\n",
    "    response_kind = output_kind[0]['generated_text'][1]['content']\n",
    "    if \"Zusammenfassung:\" in response_kind:\n",
    "        summary_kind = response_kind.replace(\"Zusammenfassung:\", \"\").strip()\n",
    "    else:\n",
    "        summary_kind = \"\"\n",
    "\n",
    "    prompt_jugendlicher = \"Fasse die folgende Rede in 1 bis 5 Sätzen so zusammen, dass ein Jugendlicher sie verstehen kann. Beginne nicht immer mit \\\"Es wird\\\" oder \\\"Der Redner\\\"!!! Schreibe die Sätze in der passiven Form. Erwähne nur den Inhalt, nicht wer was gesagt oder gefordert hat. Erwähne auch keine Parteien oder Personen. Konzentriere dich auf Forderungen, aber beginne nicht immer mit \\\"es wird gefordert\\\". Beginne die Zusammenfassung mit Zusammenfassung:\\n\"+text\n",
    "    output_jugendlicher = llm_pipeline([{\"role\": \"user\", \"content\": prompt_jugendlicher}], max_new_tokens=512, temperature=0.9, )\n",
    "    response_jugendlicher = output_jugendlicher[0]['generated_text'][1]['content']\n",
    "    if \"Zusammenfassung:\" in response_jugendlicher:\n",
    "        summary_jugendlicher = response_jugendlicher.replace(\"Zusammenfassung:\", \"\").strip()\n",
    "    else:\n",
    "        summary_jugendlicher = \"\"\n",
    "\n",
    "    prompt_erwachsener = \"Fasse die folgende Rede in 1 bis 5 Sätzen so zusammen, dass ein Erwachsener sie verstehen kann. Beginne nicht immer mit \\\"Es wird\\\" oder \\\"Der Redner\\\"!!! Schreibe die Sätze in der passiven Form. Erwähne nur den Inhalt, nicht wer was gesagt oder gefordert hat. Erwähne auch keine Parteien oder Personen. Konzentriere dich auf Forderungen, aber beginne nicht immer mit \\\"es wird gefordert\\\". Beginne die Zusammenfassung mit Zusammenfassung:\\n\"+text\n",
    "    output_erwachsener = llm_pipeline([{\"role\": \"user\", \"content\": prompt_erwachsener}], max_new_tokens=512, temperature=0.9, )\n",
    "    response_erwachsener = output_erwachsener[0]['generated_text'][1]['content']\n",
    "    if \"Zusammenfassung:\" in response_erwachsener:\n",
    "        summary_erwachsener = response_erwachsener.replace(\"Zusammenfassung:\", \"\").strip()\n",
    "    else:\n",
    "        summary_erwachsener = \"\"\n",
    "\n",
    "    prompt_eloquenter_mensch = \"Fasse die folgende Rede in 1 bis 5 Sätzen so zusammen, dass ein sehr eloquenter Mensch, der sich durch eine gehobene Wortwahl abzugrenzen versucht, sie verstehen kann. Beginne nicht immer mit \\\"Es wird\\\" oder \\\"Der Redner\\\"!!! Schreibe die Sätze in der passiven Form. Erwähne nur den Inhalt, nicht wer was gesagt oder gefordert hat. Erwähne auch keine Parteien oder Personen. Konzentriere dich auf Forderungen, aber beginne nicht immer mit \\\"es wird gefordert\\\". Beginne die Zusammenfassung mit Zusammenfassung:\\n\"+text\n",
    "    output_eloquenter_mensch = llm_pipeline([{\"role\": \"user\", \"content\": prompt_eloquenter_mensch}], max_new_tokens=512, temperature=0.9, )\n",
    "    response_eloquenter_mensch = output_eloquenter_mensch[0]['generated_text'][1]['content']\n",
    "    if \"Zusammenfassung:\" in response_eloquenter_mensch:\n",
    "        summary_eloquenter_mensch = response_eloquenter_mensch.replace(\"Zusammenfassung:\", \"\").strip()\n",
    "    else:\n",
    "        summary_eloquenter_mensch = \"\"\n",
    "\n",
    "    return summary_kind, summary_jugendlicher, summary_erwachsener, summary_eloquenter_mensch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fae4e2a-5e09-448d-8b2c-24de9805f9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_kind, summary_jugendlicher, summary_erwachsener, summary_eloquenter_mensch = get_summaries(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc30bd6e-4346-4aac-8fb3-d0db899b68c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "l_summary_kind = []\n",
    "l_summary_jugendlicher = []\n",
    "l_summary_erwachsener = []\n",
    "l_summary_eloquenter_mensch = []\n",
    "\n",
    "for text in tqdm(l_Text):\n",
    "    summary_kind, summary_jugendlicher, summary_erwachsener, summary_eloquenter_mensch = get_summaries(text)\n",
    "    \n",
    "    l_summary_kind.append(summary_kind)\n",
    "    l_summary_jugendlicher.append(summary_jugendlicher)\n",
    "    l_summary_erwachsener.append(summary_erwachsener)\n",
    "    l_summary_eloquenter_mensch.append(summary_eloquenter_mensch)\n",
    "    \n",
    "    df_temp = df_part.head(len(l_summary_kind)).copy()\n",
    "    df_temp['summary_kind'] = l_summary_kind\n",
    "    df_temp['summary_jugendlicher'] = l_summary_jugendlicher\n",
    "    df_temp['summary_erwachsener'] = l_summary_erwachsener\n",
    "    df_temp['summary_eloquenter_mensch'] = l_summary_eloquenter_mensch\n",
    "    if i%10 == 0:\n",
    "        df_temp.to_json(\"Bundestagsreden_senti_pos_neg_summarized_\"+str(i_index+1)+\"_\"+str(i_total)+\".json\")\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4850d4-af35-4af7-9398-f7fcc0a3e678",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
